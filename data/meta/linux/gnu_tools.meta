a:2:{s:7:"current";a:8:{s:4:"date";a:2:{s:7:"created";i:1488294172;s:8:"modified";i:1489410114;}s:4:"user";s:0:"";s:7:"creator";s:0:"";s:11:"last_change";a:8:{s:4:"date";i:1505991671;s:2:"ip";s:14:"150.130.96.129";s:4:"type";s:1:"e";s:2:"id";s:15:"linux:gnu_tools";s:4:"user";s:6:"tadone";s:3:"sum";s:0:"";s:5:"extra";s:0:"";s:10:"sizechange";i:209;}s:5:"title";s:3:"AWK";s:8:"relation";a:2:{s:10:"references";a:4:{s:10:"linux:find";b:1;s:9:"linux:sed";b:1;s:10:"linux:nmap";b:1;s:11:"linux:xargs";b:1;}s:10:"firstimage";s:0:"";}s:8:"internal";a:2:{s:5:"cache";b:1;s:3:"toc";b:1;}s:11:"description";a:1:{s:8:"abstract";s:250:"AWK

Print only 2 position with default delimiter


cURL

Download site/file (without saving) and grep its content:

Save file with cURL




Find

Find regular file (f) in current directory (.) and print to STDOUT on same line (no new line)


-print0";}}s:10:"persistent";a:4:{s:4:"date";a:2:{s:7:"created";i:1488294172;s:8:"modified";i:1489410114;}s:4:"user";s:0:"";s:7:"creator";s:0:"";s:11:"last_change";a:8:{s:4:"date";i:1505991671;s:2:"ip";s:14:"150.130.96.129";s:4:"type";s:1:"e";s:2:"id";s:15:"linux:gnu_tools";s:4:"user";s:6:"tadone";s:3:"sum";s:0:"";s:5:"extra";s:0:"";s:10:"sizechange";i:209;}}}